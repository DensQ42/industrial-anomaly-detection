{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ac380e4",
   "metadata": {},
   "source": [
    "### Clear memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "823d9e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79e1eb2",
   "metadata": {},
   "source": [
    "### Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7edc1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, joblib, shap, os, anthropic, uvicorn, logging, hashlib, nest_asyncio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from dotenv import load_dotenv\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema.exceptions import LangChainException\n",
    "from langchain.schema.output_parser import OutputParserException\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import ValidationError\n",
    "from datetime import datetime\n",
    "from fastapi import FastAPI, HTTPException, UploadFile, File, Form\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel, Field\n",
    "from io import StringIO\n",
    "from contextlib import asynccontextmanager\n",
    "from typing import List, Tuple, Dict, Optional, Any, Union\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e07de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab7a239c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('..')\n",
    "from src.scripts.data_utils import TEPDataLoader, filter_csv\n",
    "from src.scripts.feature_engineering import create_lag_features, create_diff_features, create_rolling_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79956f36",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Load preprocessed Tennessee Eastman Process data from the previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f9bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = TEPDataLoader(\n",
    "    raw_data_path='../data/raw',\n",
    "    processed_data_path='../data/processed',\n",
    ")\n",
    "\n",
    "# # keep this commented, if first notebook was run\n",
    "# loader.convert_and_save_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b71800f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_FAULTS = [0, 1, 13, 16]\n",
    "MAX_SIMULATION = 50\n",
    "files = ['TEP_faulty_testing']\n",
    "\n",
    "# # keep this commented, if first notebook was run\n",
    "# for f in files:\n",
    "#     filter_csv(f, SELECTED_FAULTS, MAX_SIMULATION, data_path='../data/processed')\n",
    "#     print(f'File {f} has been filtered and saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c23ee0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('../data/processed/TEP_faulty_testing_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e134e4ec",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Prepare datasets and create binary fault indicators for anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f07630fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['faultNumber'] = df_test['faultNumber'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619830f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['faulty'] = (df_test['faultNumber'] > 0) & (df_test['sample'] > 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dd074b",
   "metadata": {},
   "source": [
    "Select 3 samples from faulty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c720f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data = df_test[df_test['faulty'] == True]\n",
    "sim_data1 = sim_data.iloc[1000:1003]\n",
    "sim_data2 = sim_data.iloc[3000:3003]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977c52d9",
   "metadata": {},
   "source": [
    "# Production Pipeline Architecture Design\n",
    "This section implements a complete end-to-end anomaly detection pipeline that integrates XGBoost predictions, SHAP explanations, and LLM-generated natural language responses for industrial operators.\n",
    "\n",
    "## Basic Pipeline Implementation\n",
    "The initial pipeline demonstrates core functionality with a clean, modular design that processes raw sensor data through feature engineering, anomaly detection, explainability analysis, and natural language generation.\n",
    "\n",
    "### Pipeline Workflow\n",
    "1. **Data Preprocessing**: Extract temporal sequences and apply feature engineering\n",
    "2. **Anomaly Detection**: Generate predictions and confidence scores using trained XGBoost model\n",
    "3. **Explainability**: Calculate SHAP values and identify most impactful process variables\n",
    "4. **Natural Language Generation**: Create operator-friendly explanations via Anthropic Claude API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdc42ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetectionPipeline:\n",
    "\n",
    "    FEATURE_DESCRIPTIONS = {\n",
    "            'xmeas_1': 'A feed (stream 1)',\n",
    "            'xmeas_2': 'D feed (stream 2)',\n",
    "            'xmeas_3': 'E feed (stream 3)',\n",
    "            'xmeas_4': 'A and C feed (stream 4)',\n",
    "            'xmeas_5': 'Recycle flow (stream 8)',\n",
    "            'xmeas_6': 'Reactor feed rate (stream 6)',\n",
    "            'xmeas_7': 'Reactor pressure',\n",
    "            'xmeas_8': 'Reactor level',\n",
    "            'xmeas_9': 'Reactor temperature',\n",
    "            'xmeas_10': 'Purge rate (stream 9)',\n",
    "            'xmeas_11': 'Product separator temperature',\n",
    "            'xmeas_12': 'Product separator level',\n",
    "            'xmeas_13': 'Product separator pressure',\n",
    "            'xmeas_14': 'Product separator underflow (stream 10)',\n",
    "            'xmeas_15': 'Stripper level',\n",
    "            'xmeas_16': 'Stripper pressure',\n",
    "            'xmeas_17': 'Stripper underflow (stream 11)',\n",
    "            'xmeas_18': 'Stripper temperature',\n",
    "            'xmeas_19': 'Stripper steam flow',\n",
    "            'xmeas_20': 'Compressor work',\n",
    "            'xmeas_21': 'Reactor cooling water outlet temperature',\n",
    "            'xmeas_22': 'Separator cooling water outlet temperature',\n",
    "            'xmeas_23': 'A composition in reactor feed (stream 6)',\n",
    "            'xmeas_24': 'B composition in reactor feed (stream 6)',\n",
    "            'xmeas_25': 'C composition in reactor feed (stream 6)',\n",
    "            'xmeas_26': 'D composition in reactor feed (stream 6)',\n",
    "            'xmeas_27': 'E composition in reactor feed (stream 6)',\n",
    "            'xmeas_28': 'F composition in reactor feed (stream 6)',\n",
    "            'xmeas_29': 'A composition in purge gas (stream 9)',\n",
    "            'xmeas_30': 'B composition in purge gas (stream 9)',\n",
    "            'xmeas_31': 'C composition in purge gas (stream 9)',\n",
    "            'xmeas_32': 'D composition in purge gas (stream 9)',\n",
    "            'xmeas_33': 'E composition in purge gas (stream 9)',\n",
    "            'xmeas_34': 'F composition in purge gas (stream 9)',\n",
    "            'xmeas_35': 'G composition in purge gas (stream 9)',\n",
    "            'xmeas_36': 'H composition in purge gas (stream 9)',\n",
    "            'xmeas_37': 'D composition in product (stream 11)',\n",
    "            'xmeas_38': 'E composition in product (stream 11)',\n",
    "            'xmeas_39': 'F composition in product (stream 11)',\n",
    "            'xmeas_40': 'G composition in product (stream 11)',\n",
    "            'xmeas_41': 'H composition in product (stream 11)',\n",
    "            'xmv_1': 'D feed flow valve (stream 2)',\n",
    "            'xmv_2': 'E feed flow valve (stream 3)',\n",
    "            'xmv_3': 'A feed flow valve (stream 1)',\n",
    "            'xmv_4': 'A and C feed flow valve  (stream 4)',\n",
    "            'xmv_5': 'Compressor recycle valve',\n",
    "            'xmv_6': 'Purge valve (stream 9)',\n",
    "            'xmv_7': 'Separator pot liquid flow valve (stream 10)',\n",
    "            'xmv_8': 'Stripper liquid product flow valve (stream 11)',\n",
    "            'xmv_9': 'Stripper steam valve',\n",
    "            'xmv_10': 'Reactor cooling water flow',\n",
    "            'xmv_11': 'Condenser cooling water flow',\n",
    "            'xmv_12': 'Agitator speed',\n",
    "        }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.anomaly_detector = XGBClassifier(n_jobs=-1, random_state=42)\n",
    "        self.anomaly_detector.load_model('../models/xgb_model.json')\n",
    "\n",
    "        self.explainer = shap.TreeExplainer(self.anomaly_detector)\n",
    "\n",
    "        self.scaler = joblib.load('../models/scaler.pkl')\n",
    "\n",
    "        self.all_features = joblib.load('../models/all_features.pkl')\n",
    "        self.selected_features = joblib.load('../models/selected_features.pkl')\n",
    "\n",
    "        load_dotenv()\n",
    "        self.llm_client = anthropic.Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "    def get_sequence_for_analysis(self, data:pd.DataFrame, simulation_run:int=None, target_sample:int=None) -> pd.DataFrame:\n",
    "        if simulation_run is None:\n",
    "            simulation_run = self.simulation_run\n",
    "        if target_sample is None:\n",
    "            target_sample = self.target_sample\n",
    "\n",
    "        assert data.shape[0] >= 3, 'At least 3 samples must be provided'\n",
    "        assert target_sample >= 2, 'Target sample has to have at least 2 previous time steps'\n",
    "        sim_data = data[data['simulationRun'] == simulation_run]\n",
    "        start_idx = target_sample - 2\n",
    "        end_idx = target_sample\n",
    "        mask = (sim_data['sample'] >= start_idx) & (sim_data['sample'] <= end_idx)\n",
    "        return sim_data[mask]\n",
    "\n",
    "    def feature_engineering(self, data:pd.DataFrame) -> pd.DataFrame:\n",
    "        data = create_lag_features(data=data, lags=[1,2], columns=self.selected_features, group_by='simulationRun', dropna=False)\n",
    "        data = create_rolling_features(data=data, window_sizes=[3], columns=self.selected_features, group_by='simulationRun', dropna=False)\n",
    "        data = create_diff_features(data=data, columns=self.selected_features, group_by='simulationRun', dropna=True)\n",
    "        return data\n",
    "\n",
    "    def feature_scaling(self, data:pd.DataFrame) -> np.ndarray:\n",
    "        X = self.scaler.transform(data[self.all_features])\n",
    "        return X\n",
    "\n",
    "    def predict_anomaly(self, X:np.ndarray) -> tuple[int, float]:\n",
    "        confidence = self.anomaly_detector.predict_proba(X)[0, 1]\n",
    "        prediction = self.anomaly_detector.predict(X)[0]\n",
    "        return prediction, confidence\n",
    "\n",
    "    def get_shap_importance(self, X:np.ndarray) -> np.ndarray:\n",
    "        shap_values = self.explainer(X).values\n",
    "        return shap_values\n",
    "\n",
    "    def get_most_impactful_features(self, shap_values:np.ndarray, top_n:int=3) -> list:\n",
    "        base_feature_impacts = {}\n",
    "\n",
    "        for i, feature in enumerate(self.all_features):\n",
    "            impact = shap_values[0,i]\n",
    "            base_feature = '_'.join(feature.split('_')[:2])\n",
    "            base_description = self.FEATURE_DESCRIPTIONS[base_feature]\n",
    "\n",
    "            if base_feature not in base_feature_impacts:\n",
    "                base_feature_impacts[base_feature] = {'total_impact': 0, 'description': base_description}\n",
    "\n",
    "            base_feature_impacts[base_feature]['total_impact'] += impact\n",
    "\n",
    "        impactful_features = sorted(base_feature_impacts.items(), key=lambda x: x[1]['total_impact'], reverse=True)\n",
    "        return impactful_features[:top_n]\n",
    "\n",
    "    def create_llm_prompt(self, confidence:float=None, impactful_features:list[tuple[str, dict]]=None) -> str:\n",
    "        if confidence is None:\n",
    "            confidence = self.confidence\n",
    "        if impactful_features is None:\n",
    "            impactful_features = self.impactful_features\n",
    "\n",
    "        if confidence >= 0.75:\n",
    "            context = f'ALERT: {confidence:.1%} confidence - Key factors: '\n",
    "        elif confidence >= 0.5:\n",
    "            context = f'CAUTION: {confidence:.1%} confidence - Key factors: '\n",
    "        else:\n",
    "            context = f'NORMAL: {confidence:.1%} anomaly probability'\n",
    "\n",
    "        factors = []\n",
    "        for _, data in impactful_features:\n",
    "            description = data['description']\n",
    "            if confidence >= 0.5:\n",
    "                factors.append(description)\n",
    "\n",
    "        context += ', '.join(factors)\n",
    "        prompt = f\"\"\"You are a Tennessee Eastman Process control engineer analyzing plant anomalies. Your role is to provide actionable technical analysis for plant operators.\n",
    "\n",
    "CONTEXT: Tennessee Eastman is a chemical process with reactor, separator, stripper, and recycle streams producing products G and H from reactants A, C, D, E.\n",
    "\n",
    "ANALYSIS FORMAT:\n",
    "- STATUS: [NORMAL (less than 50% confidence) / CAUTION (between 50% and 75% confidence) / ALERT (more than 75% confidence)]\n",
    "- ISSUE: Brief technical description (one sentence)\n",
    "- ROOT CAUSE: Most likely physical/chemical cause (one sentence)\n",
    "- IMMEDIATE ACTION: Single most critical operator step\n",
    "- MONITORING: One key parameter to track\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "Input: NORMAL: 15.2% anomaly probability\n",
    "Output: STATUS: NORMAL - Continue routine monitoring of all process variables.\n",
    "\n",
    "Input: CAUTION: 65.8% confidence - Key factors: Reactor temperature, Cooling water outlet temperature\n",
    "Output: STATUS: CAUTION\n",
    "ISSUE: Reactor thermal management deviation detected\n",
    "ROOT CAUSE: Cooling water system efficiency reduction or heat duty increase\n",
    "IMMEDIATE ACTION: Verify cooling water flow rates and heat exchanger performance\n",
    "MONITORING: Track reactor temperature trend\n",
    "\n",
    "Input: ALERT: 94.3% confidence - Key factors: Product separator pressure, Product separator level, Product separator temperature\n",
    "Output: STATUS: ALERT\n",
    "ISSUE: Product separator control system failure detected\n",
    "ROOT CAUSE: Multiple control loops failing simultaneously indicating instrumentation malfunction\n",
    "IMMEDIATE ACTION: Switch separator to manual control and verify pressure relief systems\n",
    "MONITORING: Product separator pressure\n",
    "\n",
    "CURRENT ANALYSIS:\n",
    "{context}\n",
    "Output:\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def generate_explanation(self, prompt:str) -> str:\n",
    "        response = self.llm_client.messages.create(\n",
    "            model='claude-3-haiku-20240307',\n",
    "            max_tokens=200,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            temperature=0.3,\n",
    "        )\n",
    "\n",
    "        return response.content[0].text\n",
    "\n",
    "    def analyze_sample(self, data:pd.DataFrame, simulation_run:int=None, target_sample:int=None) -> str:\n",
    "        self.simulation_run = data.iloc[0]['simulationRun'] if simulation_run is None else simulation_run\n",
    "        self.target_sample = data['sample'].max() if target_sample is None else target_sample\n",
    "        data = self.get_sequence_for_analysis(data)\n",
    "        data = self.feature_engineering(data)\n",
    "        X = self.feature_scaling(data)\n",
    "        self.prediction, self.confidence = self.predict_anomaly(X)\n",
    "        shap_values = self.get_shap_importance(X)\n",
    "        self.impactful_features = self.get_most_impactful_features(shap_values)\n",
    "        prompt = self.create_llm_prompt()\n",
    "        response = self.generate_explanation(prompt)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe72a66",
   "metadata": {},
   "source": [
    "### Pipeline Validation Testing\n",
    "Validate the basic pipeline functionality using real anomaly data to ensure all components integrate correctly and produce meaningful operator guidance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707d0471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATUS: ALERT\n",
      "ISSUE: Stripper system performance degradation detected\n",
      "ROOT CAUSE: Reduced separation efficiency in the stripper column, likely due to fouling or scaling\n",
      "IMMEDIATE ACTION: Reduce A and C feed rates, increase stripper temperature setpoint to improve separation\n",
      "MONITORING: Stripper temperature and pressure profiles\n"
     ]
    }
   ],
   "source": [
    "p = AnomalyDetectionPipeline()\n",
    "r = p.analyze_sample(sim_data)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4798988",
   "metadata": {},
   "source": [
    "The explanation from LLM to provided piece of data is received - the pipeline works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df7ff2",
   "metadata": {},
   "source": [
    "## Improved Production Pipeline with LangChain Integration\n",
    "The improved version addresses production requirements with enterprise-grade features including error handling, caching, conversation memory, and robust fallback mechanisms.\n",
    "\n",
    "### Advanced Features Implementation\n",
    "\n",
    "**Conversation Memory**: maintains context of recent analyses to provide historically-aware explanations, enabling operators to understand patterns and recurring issues.\n",
    "\n",
    "**Response Caching**: implements intelligent caching based on confidence scores and feature combinations to reduce API costs and improve response times for similar scenarios.\n",
    "\n",
    "**Error Handling & Fallbacks**: comprehensive exception handling with structured fallback responses ensures system reliability even during API failures or unexpected data conditions.\n",
    "\n",
    "**Response Validation**: automatic validation of LLM outputs against expected format ensures consistent, actionable operator guidance.\n",
    "\n",
    "**Structured Logging**: detailed logging throughout the pipeline enables monitoring, debugging, and performance optimization in production environments.\n",
    "\n",
    "### LangChain Integration Benefits\n",
    "- **Prompt Management**: template-based prompt engineering with variable injection\n",
    "- **Memory Management**: automatic conversation history tracking and context management\n",
    "- **Chain Architecture**: modular LLM interaction patterns for maintainable code\n",
    "- **Provider Abstraction**: easy switching between different LLM providers if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cfe071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger('anomaly_detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e32aa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedAnomalyDetectionPipeline:\n",
    "\n",
    "    FEATURE_DESCRIPTIONS = {\n",
    "            'xmeas_1': 'A feed (stream 1)',\n",
    "            'xmeas_2': 'D feed (stream 2)',\n",
    "            'xmeas_3': 'E feed (stream 3)',\n",
    "            'xmeas_4': 'A and C feed (stream 4)',\n",
    "            'xmeas_5': 'Recycle flow (stream 8)',\n",
    "            'xmeas_6': 'Reactor feed rate (stream 6)',\n",
    "            'xmeas_7': 'Reactor pressure',\n",
    "            'xmeas_8': 'Reactor level',\n",
    "            'xmeas_9': 'Reactor temperature',\n",
    "            'xmeas_10': 'Purge rate (stream 9)',\n",
    "            'xmeas_11': 'Product separator temperature',\n",
    "            'xmeas_12': 'Product separator level',\n",
    "            'xmeas_13': 'Product separator pressure',\n",
    "            'xmeas_14': 'Product separator underflow (stream 10)',\n",
    "            'xmeas_15': 'Stripper level',\n",
    "            'xmeas_16': 'Stripper pressure',\n",
    "            'xmeas_17': 'Stripper underflow (stream 11)',\n",
    "            'xmeas_18': 'Stripper temperature',\n",
    "            'xmeas_19': 'Stripper steam flow',\n",
    "            'xmeas_20': 'Compressor work',\n",
    "            'xmeas_21': 'Reactor cooling water outlet temperature',\n",
    "            'xmeas_22': 'Separator cooling water outlet temperature',\n",
    "            'xmeas_23': 'A composition in reactor feed (stream 6)',\n",
    "            'xmeas_24': 'B composition in reactor feed (stream 6)',\n",
    "            'xmeas_25': 'C composition in reactor feed (stream 6)',\n",
    "            'xmeas_26': 'D composition in reactor feed (stream 6)',\n",
    "            'xmeas_27': 'E composition in reactor feed (stream 6)',\n",
    "            'xmeas_28': 'F composition in reactor feed (stream 6)',\n",
    "            'xmeas_29': 'A composition in purge gas (stream 9)',\n",
    "            'xmeas_30': 'B composition in purge gas (stream 9)',\n",
    "            'xmeas_31': 'C composition in purge gas (stream 9)',\n",
    "            'xmeas_32': 'D composition in purge gas (stream 9)',\n",
    "            'xmeas_33': 'E composition in purge gas (stream 9)',\n",
    "            'xmeas_34': 'F composition in purge gas (stream 9)',\n",
    "            'xmeas_35': 'G composition in purge gas (stream 9)',\n",
    "            'xmeas_36': 'H composition in purge gas (stream 9)',\n",
    "            'xmeas_37': 'D composition in product (stream 11)',\n",
    "            'xmeas_38': 'E composition in product (stream 11)',\n",
    "            'xmeas_39': 'F composition in product (stream 11)',\n",
    "            'xmeas_40': 'G composition in product (stream 11)',\n",
    "            'xmeas_41': 'H composition in product (stream 11)',\n",
    "            'xmv_1': 'D feed flow valve (stream 2)',\n",
    "            'xmv_2': 'E feed flow valve (stream 3)',\n",
    "            'xmv_3': 'A feed flow valve (stream 1)',\n",
    "            'xmv_4': 'A and C feed flow valve  (stream 4)',\n",
    "            'xmv_5': 'Compressor recycle valve',\n",
    "            'xmv_6': 'Purge valve (stream 9)',\n",
    "            'xmv_7': 'Separator pot liquid flow valve (stream 10)',\n",
    "            'xmv_8': 'Stripper liquid product flow valve (stream 11)',\n",
    "            'xmv_9': 'Stripper steam valve',\n",
    "            'xmv_10': 'Reactor cooling water flow',\n",
    "            'xmv_11': 'Condenser cooling water flow',\n",
    "            'xmv_12': 'Agitator speed',\n",
    "        }\n",
    "\n",
    "    PROMPT_TEMPLATE = \"\"\"You are a Tennessee Eastman Process control engineer analyzing plant anomalies. Your role is to provide actionable technical analysis for plant operators.\n",
    "\n",
    "CONTEXT: Tennessee Eastman is a chemical process with reactor, separator, stripper, and recycle streams producing products G and H from reactants A, C, D, E.\n",
    "\n",
    "HISTORICAL CONTEXT (last 5 relevant analyses):\n",
    "{HISTORY}\n",
    "\n",
    "ANALYSIS FORMAT:\n",
    "- STATUS: [NORMAL (less than 50% confidence) / CAUTION (between 50% and 75% confidence) / ALERT (more than 75% confidence)]\n",
    "- ISSUE: Brief technical description (one sentence)\n",
    "- ROOT CAUSE: Most likely physical/chemical cause (one sentence)\n",
    "- IMMEDIATE ACTION: Single most critical operator step\n",
    "- MONITORING: One key parameter to track\n",
    "\n",
    "EXAMPLES:\n",
    "\n",
    "Input: NORMAL: 15.2% anomaly probability\n",
    "Output: STATUS: NORMAL - Continue routine monitoring of all process variables.\n",
    "\n",
    "Input: CAUTION: 65.8% confidence - Key factors: Reactor temperature, Cooling water outlet temperature\n",
    "Output: STATUS: CAUTION\n",
    "ISSUE: Reactor thermal management deviation detected\n",
    "ROOT CAUSE: Cooling water system efficiency reduction or heat duty increase\n",
    "IMMEDIATE ACTION: Verify cooling water flow rates and heat exchanger performance\n",
    "MONITORING: Track reactor temperature trend\n",
    "\n",
    "Input: ALERT: 94.3% confidence - Key factors: Product separator pressure, Product separator level, Product separator temperature\n",
    "Output: STATUS: ALERT\n",
    "ISSUE: Product separator control system failure detected\n",
    "ROOT CAUSE: Multiple control loops failing simultaneously indicating instrumentation malfunction\n",
    "IMMEDIATE ACTION: Switch separator to manual control and verify pressure relief systems\n",
    "MONITORING: Product separator pressure\n",
    "\n",
    "CURRENT ANALYSIS:\n",
    "{CONTEXT}\n",
    "Output:\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.anomaly_detector = XGBClassifier(n_jobs=-1, random_state=42)\n",
    "        self.anomaly_detector.load_model('../models/xgb_model.json')\n",
    "\n",
    "        self.explainer = shap.TreeExplainer(self.anomaly_detector)\n",
    "        self.scaler = joblib.load('../models/scaler.pkl')\n",
    "        self.all_features = joblib.load('../models/all_features.pkl')\n",
    "        self.selected_features = joblib.load('../models/selected_features.pkl')\n",
    "\n",
    "        self.response_cache = {}\n",
    "\n",
    "        load_dotenv()\n",
    "\n",
    "        try:\n",
    "            self.llm = ChatAnthropic(\n",
    "                model='claude-3-haiku-20240307',\n",
    "                temperature=0.3,\n",
    "                max_tokens=200,\n",
    "                api_key=os.getenv('ANTHROPIC_API_KEY')\n",
    "            )\n",
    "\n",
    "            self.prompt_template = PromptTemplate(\n",
    "                template=self.PROMPT_TEMPLATE,\n",
    "                input_variables=['CONTEXT', 'HISTORY'],\n",
    "            )\n",
    "\n",
    "            self.memory = ConversationBufferWindowMemory(\n",
    "                k=3,\n",
    "                memory_key='HISTORY',\n",
    "                input_key='CONTEXT',\n",
    "                return_messages=False,\n",
    "            )\n",
    "\n",
    "            self.conversation_chain = ConversationChain(\n",
    "                llm=self.llm,\n",
    "                prompt=self.prompt_template,\n",
    "                memory=self.memory,\n",
    "                input_key='CONTEXT',\n",
    "            )\n",
    "\n",
    "            logger.info('ImprovedAnomalyDetectionPipeline initialized successfully')\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Failed to initialize LangChain components: {str(e)}')\n",
    "            raise\n",
    "\n",
    "    def get_sequence_for_analysis(self, data:pd.DataFrame, target_sample:int = None, simulation_run:int = None) -> pd.DataFrame:\n",
    "        if target_sample is None:\n",
    "            target_sample = self.target_sample\n",
    "        if simulation_run is None:\n",
    "            simulation_run = self.simulation_run\n",
    "\n",
    "        assert data.shape[0] >= 3, 'At least 3 samples must be provided'\n",
    "        assert target_sample >= 2, 'Target sample has to have at least 2 previous time steps'\n",
    "\n",
    "        sim_data = data[data['simulationRun'] == simulation_run]\n",
    "        start_idx = target_sample - 2\n",
    "        end_idx = target_sample\n",
    "        mask = (sim_data['sample'] >= start_idx) & (sim_data['sample'] <= end_idx)\n",
    "        return sim_data[mask]\n",
    "\n",
    "    def feature_engineering(self, data:pd.DataFrame) -> pd.DataFrame:\n",
    "        data = create_lag_features(data=data, lags=[1,2], columns=self.selected_features, group_by='simulationRun', dropna=False)\n",
    "        data = create_rolling_features(data=data, window_sizes=[3], columns=self.selected_features, group_by='simulationRun', dropna=False)\n",
    "        data = create_diff_features(data=data, columns=self.selected_features, group_by='simulationRun', dropna=True)\n",
    "        return data\n",
    "\n",
    "    def feature_scaling(self, data:pd.DataFrame) -> np.ndarray:\n",
    "        X = self.scaler.transform(data[self.all_features])\n",
    "        return X\n",
    "\n",
    "    def predict_anomaly(self, X:np.ndarray) -> tuple[int, float]:\n",
    "        confidence = self.anomaly_detector.predict_proba(X)[0, 1]\n",
    "        prediction = self.anomaly_detector.predict(X)[0]\n",
    "        return prediction, confidence\n",
    "\n",
    "    def get_shap_importance(self, X:np.ndarray) -> np.ndarray:\n",
    "        shap_values = self.explainer(X).values\n",
    "        return shap_values\n",
    "\n",
    "    def get_most_impactful_features(self, shap_values:np.ndarray, top_n:int=3) -> list:\n",
    "        base_feature_impacts = {}\n",
    "\n",
    "        for i, feature in enumerate(self.all_features):\n",
    "            impact = shap_values[0,i]\n",
    "            base_feature = '_'.join(feature.split('_')[:2])\n",
    "            base_description = self.FEATURE_DESCRIPTIONS[base_feature]\n",
    "\n",
    "            if base_feature not in base_feature_impacts:\n",
    "                base_feature_impacts[base_feature] = {'total_impact': 0, 'description': base_description}\n",
    "\n",
    "            base_feature_impacts[base_feature]['total_impact'] += impact\n",
    "\n",
    "        impactful_features = sorted(base_feature_impacts.items(), key=lambda x: x[1]['total_impact'], reverse=True)\n",
    "        return impactful_features[:top_n]\n",
    "\n",
    "    def prepare_context_data(self, confidence:float=None, impactful_features:list[tuple[str,dict]]=None) -> dict:\n",
    "        if confidence is None:\n",
    "            confidence = self.confidence\n",
    "        if impactful_features is None:\n",
    "            impactful_features = self.impactful_features\n",
    "\n",
    "        if confidence >= 0.75:\n",
    "            context = f'ALERT: {confidence:.1%} confidence - Key factors: '\n",
    "        elif confidence >= 0.5:\n",
    "            context = f'CAUTION: {confidence:.1%} confidence - Key factors: '\n",
    "        else:\n",
    "            context = f'NORMAL: {confidence:.1%} anomaly probability'\n",
    "\n",
    "        factors = []\n",
    "        for _, data in impactful_features:\n",
    "            description = data['description']\n",
    "            if confidence >= 0.5:\n",
    "                factors.append(description)\n",
    "\n",
    "        if factors:\n",
    "            context += ', '.join(factors)\n",
    "\n",
    "        logger.info(f'Context prepared: {context[:100]}...')\n",
    "        return {'CONTEXT': context}\n",
    "\n",
    "    def _validate_response(self, response_text: str) -> bool:\n",
    "        required_fields = ['STATUS:', 'ISSUE:', 'ROOT CAUSE:', 'IMMEDIATE ACTION:', 'MONITORING:']\n",
    "        is_valid = all(field in response_text for field in required_fields)\n",
    "        return is_valid\n",
    "\n",
    "    def _get_fallback_response(self, confidence:float=None) -> str:\n",
    "        if confidence is None:\n",
    "            confidence = self.confidence\n",
    "\n",
    "        if confidence >= 0.75:\n",
    "            responce = \"\"\"STATUS: ALERT\n",
    "ISSUE: Anomaly detected with high confidence\n",
    "ROOT CAUSE: Analysis system temporarily unavailable - manual investigation required\n",
    "IMMEDIATE ACTION: Investigate manually using plant monitoring systems\n",
    "MONITORING: All critical process parameters\"\"\"\n",
    "            return responce\n",
    "        elif confidence >= 0.5:\n",
    "            responce = \"\"\"STATUS: CAUTION\n",
    "ISSUE: Potential anomaly detected\n",
    "ROOT CAUSE: Analysis system temporarily unavailable - verify conditions manually\n",
    "IMMEDIATE ACTION: Check key process parameters for deviations\n",
    "MONITORING: Process trend monitoring\"\"\"\n",
    "            return responce\n",
    "        else:\n",
    "            responce = 'STATUS: NORMAL - Continue routine monitoring of all process variables.'\n",
    "            return responce\n",
    "\n",
    "    def _create_cache_key(self, confidence:float=None, impactful_features:list=None) -> str:\n",
    "        if confidence is None:\n",
    "            confidence = self.confidence\n",
    "        if impactful_features is None:\n",
    "            impactful_features = self.impactful_features\n",
    "\n",
    "        features_str = str(sorted(f[1]['description'] for f in impactful_features))\n",
    "        confidence_rounded = round(confidence, 2)\n",
    "\n",
    "        cache_data = f'{confidence_rounded}_{features_str}'\n",
    "        return hashlib.md5(cache_data.encode()).hexdigest()\n",
    "\n",
    "    def generate_explanation(self, context_data: dict, confidence:float=None, impactful_features:list=None) -> str:\n",
    "        if confidence is None:\n",
    "            confidence = self.confidence\n",
    "        if impactful_features is None:\n",
    "            impactful_features = self.impactful_features\n",
    "\n",
    "        cache_key = self._create_cache_key(confidence, impactful_features)\n",
    "\n",
    "        if cache_key in self.response_cache:\n",
    "            logger.info(f'Cache hit for key: {cache_key[:8]}...')\n",
    "            return self.response_cache[cache_key]\n",
    "\n",
    "        try:\n",
    "            logger.info('Invoking conversation chain with memory')\n",
    "            response = self.conversation_chain.predict(CONTEXT=context_data['CONTEXT'])\n",
    "            logger.info(f'LLM response received: {len(response)} characters')\n",
    "            if self._validate_response(response):\n",
    "                self.response_cache[cache_key] = response.strip()\n",
    "                logger.info(f'Cached response for key: {cache_key[:8]}...')\n",
    "                return response.strip()\n",
    "            else:\n",
    "                logger.error('Response validation failed, using fallback')\n",
    "                return self._get_fallback_response(confidence)\n",
    "\n",
    "        except (LangChainException, OutputParserException, ValidationError) as e:\n",
    "            logger.error(f'LangChain specific error: {str(e)}')\n",
    "            return self._get_fallback_response(confidence)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Unexpected error: {str(e)}')\n",
    "            return self._get_fallback_response(confidence)\n",
    "\n",
    "    def _extract_action_from_response(self, response: str) -> str:\n",
    "        lines = response.split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('IMMEDIATE ACTION:'):\n",
    "                return line.replace('IMMEDIATE ACTION:', '').strip()\n",
    "        return 'Monitor process parameters'\n",
    "\n",
    "    def format_analysis_for_memory(self, response:str, confidence:float=None, impactful_features:list=None, timestamp: str = None) -> str:\n",
    "        if confidence is None:\n",
    "            confidence = self.confidence\n",
    "        if impactful_features is None:\n",
    "            impactful_features = self.impactful_features\n",
    "        if timestamp is None:\n",
    "            timestamp = datetime.now().strftime('%H:%M')\n",
    "\n",
    "        features = [data['description'] for _, data in impactful_features[:3]]\n",
    "        action = self._extract_action_from_response(response)\n",
    "        memory = f\"Time: {timestamp} | Confidence: {confidence:.1%} | Features: {', '.join(features)} | Action taken: {action}\"\n",
    "        return memory\n",
    "\n",
    "    def analyze_sample(self, data: pd.DataFrame, simulation_run: int = None, target_sample: int = None) -> str:\n",
    "        try:\n",
    "            self.simulation_run = data.iloc[0]['simulationRun'] if simulation_run is None else simulation_run\n",
    "            self.target_sample = data['sample'].max() if target_sample is None else target_sample\n",
    "\n",
    "            logger.info(f'Starting analysis for simulation_run: {self.simulation_run}, target_sample: {self.target_sample}')\n",
    "\n",
    "            data = self.get_sequence_for_analysis(data)\n",
    "            data = self.feature_engineering(data)\n",
    "            X = self.feature_scaling(data)\n",
    "\n",
    "            self.prediction, self.confidence = self.predict_anomaly(X)\n",
    "            shap_values = self.get_shap_importance(X)\n",
    "\n",
    "            self.impactful_features = self.get_most_impactful_features(shap_values)\n",
    "\n",
    "            context_data = self.prepare_context_data()\n",
    "            response = self.generate_explanation(context_data)\n",
    "\n",
    "            if self.confidence >= 0.5:\n",
    "                memory_entry = self.format_analysis_for_memory(response)\n",
    "                logger.info(f'Saving to memory: {memory_entry}')\n",
    "\n",
    "            logger.info('Analysis completed successfully')\n",
    "            return response\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f'Error in analyze_sample: {str(e)}')\n",
    "            return 'STATUS: ERROR - Analysis system temporarily unavailable. Please use manual monitoring procedures.'\n",
    "\n",
    "    def reset_state(self) -> None:\n",
    "        self.prediction = None\n",
    "        self.confidence = None\n",
    "        self.impactful_features = None\n",
    "        self.simulation_run = None\n",
    "        self.target_sample = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e78e63",
   "metadata": {},
   "source": [
    "### Improved Pipeline Validation Testing\n",
    "Test the production-ready pipeline with multiple samples to validate advanced features including conversation memory, caching mechanisms, and historical context integration.\n",
    "\n",
    "**Multi-Sample Testing Strategy:**\n",
    "- Sample 1: establish baseline response and populate conversation memory\n",
    "- Sample 2: test memory integration and contextual awareness in explanations\n",
    "- Cache Validation: verify identical scenarios utilize cached responses for efficiency\n",
    "- Memory Persistence: confirm conversation history maintains relevant context across analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "281306cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomaly_detector:ImprovedAnomalyDetectionPipeline initialized successfully\n",
      "INFO:anomaly_detector:Starting analysis for simulation_run: 1.0, target_sample: 363\n",
      "INFO:anomaly_detector:Context prepared: ALERT: 100.0% confidence - Key factors: Stripper temperature, Stripper pressure, Stripper steam valv...\n",
      "INFO:anomaly_detector:Invoking conversation chain with memory\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:anomaly_detector:LLM response received: 306 characters\n",
      "INFO:anomaly_detector:Cached response for key: 96fe58ad...\n",
      "INFO:anomaly_detector:Saving to memory: Time: 01:17 | Confidence: 100.0% | Features: Stripper temperature, Stripper pressure, Stripper steam valve | Action taken: Manually control stripper steam valve to maintain temperature and pressure setpoints\n",
      "INFO:anomaly_detector:Analysis completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First: STATUS: ALERT\n",
      "ISSUE: Stripper system malfunction detected\n",
      "ROOT CAUSE: Stripper steam valve failure leading to loss of temperature and pressure control\n",
      "IMMEDIATE ACTION: Manually control stripper steam valve to maintain temperature and pressure setpoints\n",
      "MONITORING: Stripper temperature and pressure trends\n"
     ]
    }
   ],
   "source": [
    "p = ImprovedAnomalyDetectionPipeline()\n",
    "\n",
    "result1 = p.analyze_sample(sim_data1)\n",
    "print('First:', result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8df2397c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomaly_detector:Starting analysis for simulation_run: 2.0, target_sample: 763\n",
      "INFO:anomaly_detector:Context prepared: ALERT: 100.0% confidence - Key factors: Stripper temperature, A and C feed (stream 4), Stripper stea...\n",
      "INFO:anomaly_detector:Invoking conversation chain with memory\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:anomaly_detector:LLM response received: 395 characters\n",
      "INFO:anomaly_detector:Cached response for key: 2ceb047f...\n",
      "INFO:anomaly_detector:Saving to memory: Time: 01:17 | Confidence: 100.0% | Features: Stripper temperature, A and C feed (stream 4), Stripper steam valve | Action taken: Manually control the stripper steam valve to maintain the stripper temperature and pressure at their setpoints\n",
      "INFO:anomaly_detector:Analysis completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second: STATUS: ALERT\n",
      "ISSUE: Stripper system malfunction detected\n",
      "ROOT CAUSE: Stripper steam valve failure leading to loss of temperature and pressure control in the stripper\n",
      "IMMEDIATE ACTION: Manually control the stripper steam valve to maintain the stripper temperature and pressure at their setpoints\n",
      "MONITORING: Closely monitor the stripper temperature and pressure trends to ensure stable operation\n"
     ]
    }
   ],
   "source": [
    "result2 = p.analyze_sample(sim_data2)\n",
    "print('Second:', result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88fb76a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History: Human: ALERT: 100.0% confidence - Key factors: Stripper temperature, Stripper pressure, Stripper steam valve\n",
      "AI: STATUS: ALERT\n",
      "ISSUE: Stripper system malfunction detected\n",
      "ROOT CAUSE: Stripper steam valve failure leading to loss of temperature and pressure control\n",
      "IMMEDIATE ACTION: Manually control stripper steam valve to maintain temperature and pressure setpoints\n",
      "MONITORING: Stripper temperature and pressure trends\n",
      "Human: ALERT: 100.0% confidence - Key factors: Stripper temperature, A and C feed (stream 4), Stripper steam valve\n",
      "AI: STATUS: ALERT\n",
      "ISSUE: Stripper system malfunction detected\n",
      "ROOT CAUSE: Stripper steam valve failure leading to loss of temperature and pressure control in the stripper\n",
      "IMMEDIATE ACTION: Manually control the stripper steam valve to maintain the stripper temperature and pressure at their setpoints\n",
      "MONITORING: Closely monitor the stripper temperature and pressure trends to ensure stable operation\n"
     ]
    }
   ],
   "source": [
    "print('History:', p.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70733bef",
   "metadata": {},
   "source": [
    "History is being saved correctly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade4747",
   "metadata": {},
   "source": [
    "Save one sample for the future tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c79f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_data1.to_csv('../data/processed/TEP_API_test.csv', encoding='utf-8', sep=',', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c192127b",
   "metadata": {},
   "source": [
    "# API Development\n",
    "This section implements an API service that transforms the anomaly detection pipeline into a production-ready web service. The API accepts CSV files containing Tennessee Eastman Process data and returns comprehensive JSON responses including predictions, explanations, and metadata.\n",
    "\n",
    "## API Architecture Overview\n",
    "The service provides a complete interface for industrial anomaly detection with the following key capabilities:\n",
    "- **File-based Input**: CSV upload handling for time series data\n",
    "- **Comprehensive Validation**: Multi-layer data validation before processing\n",
    "- **Structured Responses**: JSON outputs with predictions, confidence scores, and explanations\n",
    "- **Production Features**: Health checks, monitoring, and error handling\n",
    "- **Documentation**: Auto-generated API documentation via FastAPI\n",
    "\n",
    "## Logging Configuration\n",
    "Establish structured logging for API operations to enable monitoring, debugging, and audit trails in production deployments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bf4b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "api_logger = logging.getLogger('anomaly_api')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd496f85",
   "metadata": {},
   "source": [
    "## API Data Models with Pydantic Validation\n",
    "Pydantic models provide automatic data validation, serialization, and API documentation generation. Each model corresponds to a specific endpoint response, ensuring type safety and consistent data structures across the API.\n",
    "\n",
    "### Model Information Endpoint (`/info`)\n",
    "The `ModelInfo` model provides comprehensive metadata about the deployed ML system, enabling clients to understand model capabilities, requirements, and compatibility. This information is essential for integration teams and system administrators.\n",
    "\n",
    "**Key Information Provided:**\n",
    "- **Model Architecture**: Details about the XGBoost + SHAP + LLM pipeline\n",
    "- **Feature Requirements**: Exact count and structure of expected input features\n",
    "- **Data Format Specifications**: Precise CSV format requirements for successful processing\n",
    "- **Training Context**: Background on model training data for confidence assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddbe912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInfo(BaseModel):\n",
    "    \"\"\"Information model for describing ML model configuration and requirements.\"\"\"\n",
    "    model_type: str = Field(..., description='Type of ML model')\n",
    "    features_count: int = Field(..., description='Number of input features')\n",
    "    llm_model: str = Field(..., description='LLM model used for explanations')\n",
    "    training_data: str = Field(..., description='Training dataset description')\n",
    "    expected_csv_format: Dict[str, Any] = Field(..., description='Expected CSV format and requirements')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11478825",
   "metadata": {},
   "source": [
    "### Health Check Endpoint (`/health`)\n",
    "The `HealthResponse` model enables monitoring systems to verify service availability and component status. Critical for production deployment where external monitoring systems need to assess API health.\n",
    "\n",
    "**Health Indicators:**\n",
    "- **Overall Status**: Aggregated service health (healthy/degraded/unhealthy)\n",
    "- **Component Status**: Individual status of ML model and LLM service\n",
    "- **Timestamp**: Enables tracking of health check frequency and response times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f85fb018",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HealthResponse(BaseModel):\n",
    "    \"\"\"Response model for API health check endpoints.\"\"\"\n",
    "    status: str = Field(..., description='Service status')\n",
    "    timestamp: str = Field(..., description='Health check timestamp')\n",
    "    model_loaded: bool = Field(..., description='Whether ML model is loaded')\n",
    "    llm_available: bool = Field(..., description='Whether LLM service is available')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28593181",
   "metadata": {},
   "source": [
    "### CSV Validation Endpoint (`/validate-csv`)\n",
    "The `CSVValidationResponse` model provides comprehensive pre-analysis validation, preventing processing failures and providing clear feedback on data issues before costly analysis operations.\n",
    "\n",
    "**Validation Features:**\n",
    "- **Format Compliance**: Ensures CSV structure matches Tennessee Eastman Process requirements\n",
    "- **Data Quality Checks**: Identifies missing values, incorrect data types, and structural issues\n",
    "- **Metadata Extraction**: Provides insights into data characteristics and detected parameters\n",
    "- **Warning System**: Non-blocking warnings for suboptimal but processable data conditions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97f1fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVValidationResponse(BaseModel):\n",
    "    \"\"\"Response model for CSV file validation results.\"\"\"\n",
    "    is_valid: bool = Field(..., description='Whether CSV format is valid')\n",
    "    errors: List[str] = Field(..., description='List of validation errors if any')\n",
    "    warnings: List[str] = Field(..., description='List of warnings')\n",
    "    data_shape: Optional[tuple] = Field(default=None, description='Shape of the data (rows, columns)')\n",
    "    detected_simulation_runs: Optional[List[int]] = Field(default=None, description='Detected simulation runs')\n",
    "    sample_range: Optional[Dict[str, int]] = Field(default=None, description='Min and max sample values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f5481",
   "metadata": {},
   "source": [
    "### Analysis Endpoint (`/analyze`)\n",
    "The `AnalysisResponse` model delivers comprehensive anomaly detection results with complete traceability and metadata. This is the primary deliverable for industrial monitoring systems.\n",
    "\n",
    "**Response Components:**\n",
    "- **Core Predictions**: Binary classification and confidence scores for decision making\n",
    "- **Explainability Data**: SHAP-derived feature importance for technical understanding\n",
    "- **Natural Language**: LLM-generated explanations for operator guidance  \n",
    "- **Processing Metadata**: Performance metrics and traceability information\n",
    "- **Input Context**: Reference information linking results back to source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "761aec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalysisResponse(BaseModel):\n",
    "    \"\"\"Response model for anomaly detection analysis API endpoints.\"\"\"\n",
    "    prediction: int = Field(..., ge=0, le=1, description='Binary prediction: 0=normal, 1=anomaly')\n",
    "    confidence: float = Field(..., ge=0.0, le=1.0, description='Prediction confidence score')\n",
    "    important_features: List[Dict[str, Any]] = Field(..., description='Most impactful process variables')\n",
    "    explanation: str = Field(..., description='Human-readable explanation from LLM')\n",
    "    timestamp: str = Field(..., description='Analysis timestamp')\n",
    "    model_version: str = Field(default='1.0', description='Model version used')\n",
    "    processing_time_ms: Optional[float] = Field(default=None, description='Processing time in milliseconds')\n",
    "    input_rows_count: int = Field(..., description='Number of input data rows processed')\n",
    "    simulation_run: int = Field(..., description='Simulation run identifier from data')\n",
    "    target_sample: int = Field(..., description='Target time sample analyzed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a923e40",
   "metadata": {},
   "source": [
    "## Utility Functions for Data Processing\n",
    "Essential helper functions that transform pipeline outputs into API-compliant formats and validate incoming data before processing.\n",
    "\n",
    "### SHAP Feature Formatting\n",
    "Converts internal SHAP analysis results into standardized JSON format suitable for API responses and client consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5bec2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_shap_features(impactful_features: List[tuple]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Converts SHAP feature importance data into a standardized API response format.\"\"\"\n",
    "    formatted_features = []\n",
    "    for feature_name, feature_data in impactful_features:\n",
    "        formatted_features.append({\n",
    "            'variable_name': feature_name,\n",
    "            'description': feature_data['description'],\n",
    "            'importance_score': round(float(feature_data['total_impact']), 4)\n",
    "        })\n",
    "    return formatted_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67e2a17",
   "metadata": {},
   "source": [
    "### CSV Data Validation  \n",
    "Comprehensive validation logic that ensures uploaded files meet Tennessee Eastman Process requirements before expensive ML processing begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb2afe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_csv_data(df: pd.DataFrame) -> Tuple[bool, List[str], List[str]]:\n",
    "    \"\"\"Validates CSV data format and content for Tennessee Eastman Process anomaly detection.\"\"\"\n",
    "    errors = []\n",
    "    warns = []\n",
    "\n",
    "    if len(df) < 3:\n",
    "        errors.append('CSV must contain at least 3 rows for temporal analysis')\n",
    "        return False, errors, warns\n",
    "\n",
    "    required_columns = ['sample', 'simulationRun']\n",
    "    missing_required = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_required:\n",
    "        errors.append(f'Missing required columns: {missing_required}')\n",
    "\n",
    "    required_features = ['xmeas_' + str(i) for i in range(1,42,1)] + ['xmv_' + str(i) for i in range(1,12,1)]\n",
    "    missing_required = [col for col in required_features if col not in df.columns]\n",
    "    if missing_required:\n",
    "        errors.append(f'Missing required features: {missing_required}')\n",
    "\n",
    "    non_numeric_cols = []\n",
    "    for col in required_features:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            non_numeric_cols.append(col)\n",
    "\n",
    "    if non_numeric_cols:\n",
    "        errors.append(f'Non-numeric columns found: {non_numeric_cols[:3]}...')\n",
    "\n",
    "    critical_missing = []\n",
    "    for col in ['sample', 'simulationRun']:\n",
    "        if col in df.columns and df[col].isnull().any():\n",
    "            critical_missing.append(col)\n",
    "\n",
    "    if critical_missing:\n",
    "        errors.append(f'Missing values in critical columns: {critical_missing}')\n",
    "\n",
    "    if 'sample' in df.columns:\n",
    "        if not df['sample'].is_monotonic_increasing:\n",
    "            warns.append('Sample values are not in ascending order')\n",
    "\n",
    "    if 'simulationRun' in df.columns:\n",
    "        unique_runs = df['simulationRun'].nunique()\n",
    "        if unique_runs > 1:\n",
    "            warns.append(f'Multiple simulation runs detected ({unique_runs}). Using the first one.')\n",
    "\n",
    "    is_valid = len(errors) == 0\n",
    "    return is_valid, errors, warns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb24aa97",
   "metadata": {},
   "source": [
    "## FastAPI Application Configuration\n",
    "Configure the FastAPI application with proper lifecycle management, documentation, and middleware for production deployment.\n",
    "\n",
    "### Application Lifecycle Management\n",
    "Implement proper startup and shutdown procedures to ensure the ML pipeline is initialized once during server startup and cleaned up gracefully during shutdown. This prevents repeated model loading and ensures resource management.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b8ce8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@asynccontextmanager\n",
    "async def lifespan(_app: FastAPI):\n",
    "    global pipeline\n",
    "    try:\n",
    "        api_logger.info('Initializing Anomaly Detection Pipeline...')\n",
    "        pipeline = ImprovedAnomalyDetectionPipeline()\n",
    "        api_logger.info('Pipeline initialized successfully')\n",
    "    except Exception as e:\n",
    "        api_logger.error(f'Failed to initialize pipeline: {str(e)}')\n",
    "        raise e\n",
    "\n",
    "    yield\n",
    "    api_logger.info('Shutting down pipeline...')\n",
    "    pipeline = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3db0b5d",
   "metadata": {},
   "source": [
    "### Global Pipeline State\n",
    "Maintain a global reference to the initialized pipeline for access across all endpoint handlers. This singleton pattern ensures consistent model state across requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41d7c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e16e1e3",
   "metadata": {},
   "source": [
    "### API Metadata and Documentation\n",
    "Configure comprehensive API metadata including title, description, and version for automatic documentation generation. FastAPI automatically creates interactive documentation at `/docs` and `/redoc` endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "076fcdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(\n",
    "    title='Tennessee Eastman Process Anomaly Detection API',\n",
    "    description='ML-powered system for industrial process anomaly detection and LLM explanations',\n",
    "    version='1.0.0',\n",
    "    docs_url='/docs',\n",
    "    redoc_url='/redoc',\n",
    "    lifespan=lifespan,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9829e262",
   "metadata": {},
   "source": [
    "### CORS Configuration for Development\n",
    "Enable Cross-Origin Resource Sharing for development and testing purposes. This allows web applications from different domains to interact with the API during development phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fba23225",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=['*'],  # in production this must be specified\n",
    "    allow_credentials=True,\n",
    "    allow_methods=['GET', 'POST'],\n",
    "    allow_headers=['*'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c6bb2",
   "metadata": {},
   "source": [
    "## API Endpoint Implementation\n",
    "Complete set of RESTful endpoints providing comprehensive anomaly detection services with proper error handling, validation, and monitoring capabilities.\n",
    "\n",
    "### Model Information Endpoint (`GET /info`)\n",
    "Provides essential metadata about the deployed model system, enabling clients to understand API capabilities and data requirements before integration. Returns detailed specifications for CSV format, feature requirements, and model architecture information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8541ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get('/info', response_model=ModelInfo)\n",
    "async def model_info():\n",
    "    \"\"\"Provides comprehensive information about the ML model configuration and requirements.\"\"\"\n",
    "    try:\n",
    "        if pipeline is None:\n",
    "            raise HTTPException(status_code=503, detail='Pipeline not initialized')\n",
    "\n",
    "        expected_format = {\n",
    "            'file_format': 'CSV',\n",
    "            'encoding': 'UTF-8',\n",
    "            'minimum_rows': 3,\n",
    "            'required_columns': ['sample', 'simulationRun'],\n",
    "            'feature_variables': {\n",
    "                'measured': 'xmeas_1 to xmeas_41 (41 variables)',\n",
    "                'manipulated': 'xmv_1 to xmv_11 (11 variables)'\n",
    "            },\n",
    "            'data_types': 'All process variables must be numeric',\n",
    "            'temporal_requirement': 'Data must represent 3 consecutive time points'\n",
    "        }\n",
    "\n",
    "        return ModelInfo(\n",
    "            model_type='XGBoost Classifier with SHAP explanations',\n",
    "            features_count=len(pipeline.all_features),\n",
    "            llm_model=pipeline.llm.model,\n",
    "            training_data='Tennessee Eastman Process Dataset (50 simulations for each fault type, 3 fault types in total)',\n",
    "            expected_csv_format=expected_format,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        api_logger.error(f'Error getting model info: {str(e)}')\n",
    "        raise HTTPException(status_code=500, detail=f'Failed to get model info: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a65cb5",
   "metadata": {},
   "source": [
    "### Health Monitoring Endpoint (`GET /health`)\n",
    "Critical for production deployment monitoring. Returns structured health status enabling load balancers and monitoring systems to assess service availability. Implements graceful degradation status when components are partially available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13581772",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get('/health', response_model=HealthResponse)\n",
    "async def health_check():\n",
    "    \"\"\"Provides comprehensive health status information for the anomaly detection service.\"\"\"\n",
    "    try:\n",
    "        model_loaded = pipeline is not None\n",
    "\n",
    "        llm_available = model_loaded and hasattr(pipeline, 'llm') and pipeline.llm is not None\n",
    "\n",
    "        if model_loaded and llm_available:\n",
    "            overall_status = 'healthy'\n",
    "        else:\n",
    "            overall_status = 'degraded'\n",
    "\n",
    "        return HealthResponse(\n",
    "            status=overall_status,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            model_loaded=model_loaded,\n",
    "            llm_available=llm_available,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        api_logger.error(f'Health check failed: {str(e)}')\n",
    "        return HealthResponse(\n",
    "            status='unhealthy',\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            model_loaded=False,\n",
    "            llm_available=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f997c3e2",
   "metadata": {},
   "source": [
    "### Data Validation Endpoint (`POST /validate-csv`)\n",
    "Pre-analysis validation service that checks CSV compatibility without performing expensive ML operations. Provides detailed feedback on data issues, enabling clients to fix problems before submitting for analysis. Returns comprehensive metadata about detected data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d176c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post('/validate-csv', response_model=CSVValidationResponse)\n",
    "async def validate_csv_file(file: UploadFile = File(...)):\n",
    "    \"\"\"Validates uploaded CSV files for Tennessee Eastman Process anomaly detection compatibility.\"\"\"\n",
    "    try:\n",
    "        if not file.filename.lower().endswith('.csv'):\n",
    "            return CSVValidationResponse(\n",
    "                is_valid=False,\n",
    "                errors=['File must be a CSV file'],\n",
    "                warnings=[],\n",
    "            )\n",
    "\n",
    "        contents = await file.read()\n",
    "        csv_string = contents.decode('utf-8')\n",
    "        df = pd.read_csv(StringIO(csv_string))\n",
    "\n",
    "        is_valid, errors, warns = validate_csv_data(df)\n",
    "\n",
    "        if 'simulationRun' in df.columns:\n",
    "            simulation_runs = df['simulationRun'].unique().tolist()\n",
    "        else:\n",
    "            simulation_runs = []\n",
    "\n",
    "        if 'sample' in df.columns:\n",
    "            sample_range = {'min': int(df['sample'].min()), 'max': int(df['sample'].max())}\n",
    "        else:\n",
    "            sample_range = None\n",
    "\n",
    "        return CSVValidationResponse(\n",
    "            is_valid=is_valid,\n",
    "            errors=errors,\n",
    "            warnings=warns,\n",
    "            data_shape=(len(df), len(df.columns)),\n",
    "            detected_simulation_runs=simulation_runs,\n",
    "            sample_range=sample_range\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return CSVValidationResponse(\n",
    "            is_valid=False,\n",
    "            errors=[f'Failed to process file: {str(e)}'],\n",
    "            warnings=[],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b3bb0d",
   "metadata": {},
   "source": [
    "### Primary Analysis Endpoint (`POST /analyze`)\n",
    "The core service endpoint that executes the complete anomaly detection pipeline. Accepts CSV files with optional parameters for simulation run and target sample selection. Returns comprehensive analysis including predictions, explanations, feature importance, and processing metadata.\n",
    "\n",
    "**Key Features:**\n",
    "- **Flexible Input Handling**: supports optional parameters for multi-simulation datasets\n",
    "- **Comprehensive Validation**: multi-layer validation before expensive processing\n",
    "- **Performance Monitoring**: built-in timing and logging for production monitoring\n",
    "- **Detailed Output**: complete analysis results with traceability information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bcb2aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.post('/analyze', response_model=AnalysisResponse)\n",
    "async def analyze_csv_data(\n",
    "    file: UploadFile = File(..., description='CSV file with 3 consecutive time points'),\n",
    "    simulation_run: Optional[Union[int, str, None]] = Form(default=None, description='Simulation run to analyze (if multiple in file)'),\n",
    "    target_sample: Optional[Union[int, str, None]] = Form(default=None, description='Target sample to analyze'),\n",
    "):\n",
    "    \"\"\"Performs comprehensive anomaly detection analysis on uploaded Tennessee Eastman Process data.\"\"\"\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    if simulation_run == '':\n",
    "        simulation_run = None\n",
    "    if target_sample == '':\n",
    "        target_sample = None\n",
    "\n",
    "    try:\n",
    "        if pipeline is None:\n",
    "            raise HTTPException(status_code=503, detail='Analysis pipeline not available. Please try again later.')\n",
    "\n",
    "        if not file.filename.lower().endswith('.csv'):\n",
    "            raise HTTPException(status_code=400, detail='File must be a CSV file')\n",
    "\n",
    "        api_logger.info(f'Received CSV file: {file.filename} ({file.size} bytes)')\n",
    "\n",
    "        try:\n",
    "            contents = await file.read()\n",
    "            csv_string = contents.decode('utf-8')\n",
    "            df = pd.read_csv(StringIO(csv_string))\n",
    "\n",
    "            api_logger.info(f'CSV loaded successfully: {df.shape} shape')\n",
    "\n",
    "        except UnicodeDecodeError:\n",
    "            raise HTTPException(status_code=400, detail='Invalid CSV encoding. Please use UTF-8 encoding.')\n",
    "\n",
    "        except pd.errors.EmptyDataError:\n",
    "            raise HTTPException(status_code=400, detail='CSV file is empty')\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=400, detail=f'Failed to parse CSV: {str(e)}')\n",
    "\n",
    "        is_valid, errors, warns = validate_csv_data(df)\n",
    "\n",
    "        if not is_valid:\n",
    "            raise HTTPException(status_code=400, detail=f\"CSV validation failed: {'; '.join(errors)}\")\n",
    "\n",
    "        for warning in warns:\n",
    "            api_logger.warning(warning)\n",
    "\n",
    "        api_logger.info(f'Analysis parameters: simulation_run={simulation_run}, target_sample={target_sample}')\n",
    "\n",
    "        explanation_text = pipeline.analyze_sample(\n",
    "            data=df,\n",
    "            simulation_run=simulation_run,\n",
    "            target_sample=target_sample,\n",
    "        )\n",
    "\n",
    "        simulation_run = getattr(pipeline, 'simulation_run', 0)\n",
    "        target_sample = getattr(pipeline, 'target_sample', 0)\n",
    "        impactful_features = getattr(pipeline, 'impactful_features', [])\n",
    "        prediction = getattr(pipeline, 'prediction', 0)\n",
    "        confidence = getattr(pipeline, 'confidence', 0.0)\n",
    "\n",
    "        formatted_features = format_shap_features(impactful_features)\n",
    "\n",
    "        processing_time = (datetime.now() - start_time).total_seconds() * 1000\n",
    "\n",
    "        response = AnalysisResponse(\n",
    "            prediction=int(prediction),\n",
    "            confidence=round(float(confidence), 4),\n",
    "            important_features=formatted_features,\n",
    "            explanation=explanation_text,\n",
    "            timestamp=datetime.now().isoformat(),\n",
    "            processing_time_ms=round(processing_time, 2),\n",
    "            input_rows_count=len(df),\n",
    "            simulation_run=simulation_run,\n",
    "            target_sample=target_sample,\n",
    "        )\n",
    "\n",
    "        api_logger.info(f'Analysis completed successfully in {processing_time:.2f}ms')\n",
    "        return response\n",
    "\n",
    "    except HTTPException:\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        api_logger.error(f'Unexpected error in analysis: {str(e)}')\n",
    "        raise HTTPException(status_code=500, detail=f'Analysis failed: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b16e85",
   "metadata": {},
   "source": [
    "### API Discovery Endpoint (`GET /`)\n",
    "Root endpoint providing API overview and navigation assistance for developers and integration teams. Returns structured information about available endpoints and their purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d2b36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get('/')\n",
    "async def root():\n",
    "    \"\"\"Root endpoint providing API overview and navigation information.\"\"\"\n",
    "    r = {\n",
    "        'message': 'Tennessee Eastman Process Anomaly Detection API (CSV-based)',\n",
    "        'version': '1.0.0',\n",
    "        'documentation': '/docs',\n",
    "        'health': '/health',\n",
    "        'input_format': 'CSV files with 3+ consecutive time points',\n",
    "        'endpoints': {\n",
    "            'analyze': 'POST /analyze - Main anomaly analysis endpoint (CSV upload)',\n",
    "            'validate-csv': 'POST /validate-csv - Validate CSV format before analysis',\n",
    "            'health': 'GET /health - Service health check',\n",
    "            'info': 'GET /info - Model and format information'\n",
    "        }\n",
    "    }\n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738d390",
   "metadata": {},
   "source": [
    "## Server Deployment and Testing\n",
    "Configure and launch the FastAPI server for local development and testing with appropriate server parameters and monitoring capabilities.\n",
    "\n",
    "### Server Configuration Function\n",
    "Utility function that encapsulates server startup with configurable parameters for different deployment environments. Provides clear console output for development teams to access API endpoints and documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5666ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_api_server(app, host: str = '127.0.0.1', port: int = 8000, reload: bool = False):\n",
    "    print(f'Starting CSV-based Anomaly Detection API server...')\n",
    "    print(f'Server will be available at: http://{host}:{port}')\n",
    "    print(f'API documentation: http://{host}:{port}/docs')\n",
    "    print(f'Upload CSV files at: http://{host}:{port}/analyze')\n",
    "\n",
    "    uvicorn.run(\n",
    "        app=app,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        reload=reload,\n",
    "        log_level='info',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad683fa",
   "metadata": {},
   "source": [
    "### Development Server Launch\n",
    "Initialize the server using uvicorn with nest_asyncio compatibility for Jupyter notebook environments. The server becomes immediately available for testing with interactive documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26f5918c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [13284]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:anomaly_api:Initializing Anomaly Detection Pipeline...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CSV-based Anomaly Detection API server...\n",
      "Server will be available at: http://127.0.0.1:8000\n",
      "API documentation: http://127.0.0.1:8000/docs\n",
      "Upload CSV files at: http://127.0.0.1:8000/analyze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomaly_detector:ImprovedAnomalyDetectionPipeline initialized successfully\n",
      "INFO:anomaly_api:Pipeline initialized successfully\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65422 - \"GET / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65422 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:65423 - \"GET /docs HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65423 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65423 - \"GET /info HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65437 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65486 - \"POST /validate-csv HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:anomaly_api:Received CSV file: TEP_API_test.csv (1619 bytes)\n",
      "INFO:anomaly_api:CSV loaded successfully: (3, 56) shape\n",
      "INFO:anomaly_api:Analysis parameters: simulation_run=None, target_sample=None\n",
      "INFO:anomaly_detector:Starting analysis for simulation_run: 1.0, target_sample: 363\n",
      "INFO:anomaly_detector:Context prepared: ALERT: 100.0% confidence - Key factors: Stripper temperature, Stripper pressure, Stripper steam valv...\n",
      "INFO:anomaly_detector:Invoking conversation chain with memory\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:anomaly_detector:LLM response received: 322 characters\n",
      "INFO:anomaly_detector:Cached response for key: 96fe58ad...\n",
      "INFO:anomaly_detector:Saving to memory: Time: 01:18 | Confidence: 100.0% | Features: Stripper temperature, Stripper pressure, Stripper steam valve | Action taken: Isolate the stripper system, shut down the affected process train, and initiate emergency procedures\n",
      "INFO:anomaly_detector:Analysis completed successfully\n",
      "INFO:anomaly_api:Analysis completed successfully in 1028.27ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65512 - \"POST /analyze HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65533 - \"GET / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:anomaly_api:Shutting down pipeline...\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [13284]\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "run_api_server(app)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12158faf",
   "metadata": {},
   "source": [
    "Everything works as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ce6aa3",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Complete Production Pipeline Achievement\n",
    "This notebook successfully demonstrates the transformation of research-grade anomaly detection models into a production-ready API service. The implementation bridges the gap between ML experimentation and industrial deployment through comprehensive API design with FastAPI.\n",
    "\n",
    "## Technical Integration Success\n",
    "The project achieves seamless integration of multiple complex components:\n",
    "- **XGBoost Model**: high-performance anomaly detection with 97.4% F1-score\n",
    "- **SHAP Analysis**: real-time feature importance calculation for explainability\n",
    "- **LLM Integration**: natural language explanations via Anthropic Claude API\n",
    "- **Data Validation**: multi-layer CSV validation ensuring data quality\n",
    "- **Error Handling**: robust error management with graceful degradation\n",
    "\n",
    "## Memory and Caching Implementation\n",
    "\n",
    "This project implements LangChain ConversationBufferWindowMemory to provide contextual anomaly analysis. The system remembers the last 5 analyses and can reference previous incidents when providing recommendations.\n",
    "\n",
    "**Current scope:** Memory persists within a single Python session, making it ideal for:\n",
    "- Batch processing of historical data\n",
    "- Interactive analysis sessions\n",
    "- Development and testing\n",
    "\n",
    "**Production considerations:** For stateless API deployment, memory and caching would require external storage:\n",
    "- Redis/database for persistent conversation history\n",
    "- Distributed caching for ML inference results\n",
    "- Session management for user-specific contexts\n",
    "\n",
    "\n",
    "The resulting system provides a complete solution for industrial process monitoring with operator-friendly explanations, demonstrating the practical application of advanced ML techniques in real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anomaly_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
